## Now the data are QC'd and separated into sample, I will use bwa to map against the reference and then the stacks pipeline to call
## loci. 

The overall goal is to build multiple catalogues, each with different combinations of parameters, using: (1) 20 samples per sampling 
location, in order to give a good representation of alleles, and (2) 10 replicate samples that have been run twice, in order to get 
an estimate of the error rate. 

The error rate, in addition to statistics like mean coverage per locus will help determine which combination of parameters is
optimal for my dataset.

The first step is to align the QC'd reads against the genome using bwa. I separate my samples by sampling location, as it means
I find it easier to keep track of what I'm doing and is easily scalable; starting with 20 here but will end up with 60 samples
when I optimise the pipeline. It's also because it makes looping easier, due to where the samples are stored.

Bash script for aligning with bwa:
################################################################################################################################################
#!/bin/bash 
#$ -cwd 
#$ -j y
#$ -S /bin/bash 
#$ -V
#$ -N BZalign2 ## job name

#$ -q all.q

# load bwa
module load bwa

for opt in BZ_BR00EA51 BZ_BR00EA91S BZ_BR00SA65 BZ_BR99EA22 BZ_BR99EA41 BZ_BREA103 BZ_BREA104 BZ_BREA105 BZ_BREA11 BZ_BREA46 BZ_BREA53 BZ_BREA63 BZ_BREA71 BZ_BREA81 BZ_BREA82 BZ_BREA86 BZ_BREA88 BZ_BREA96 BZ_BREA98 BZ_BREA9909
	do
		## shout out to sample being analysed
		echo now analysing ${opt}

		# align paired end reads to SRW genome
		bwa mem /storage/home/users/elc6/stacksv2/SRWgenome/RWref -t 5 \
		/storage/home/users/elc6/stacksv2/demup/${opt}.1.fq.gz \
		/storage/home/users/elc6/stacksv2/demup/${opt}.2.fq.gz \
		> /storage/home/users/elc6/stacksv2/align/${opt}_aln.sam
done
################################################################################################################################################

In order to be able to build a stacks catalogue from the mapped reads, we need to do a few 'house keeping' steps.
The first is to convert the sam file to a bam file, then we sort reads by mapping position, both using samtools. Then we add read
groups, or a way to identify a set of reads that were generated from a single run of a sequencing instrument, to the files.

################################################################################################################################################
#!/bin/bash 
#$ -cwd 
#$ -j y
#$ -S /bin/bash 
#$ -V
#$ -N BZaligninfo ## job name

#$ -q all.q

# load 
#module load samtools/1.6
samtools=/shelf/modulefiles/tools/samtools/1.6/bin/samtools
module load picard-tools
module load stacks/2.0Beta9

for opt in BZ_BR00EA51 BZ_BR00EA91 BZ_BR00SA65 BZ_BR99EA22 BZ_BR99EA41 BZ_BREA103 BZ_BREA104 BZ_BREA105 BZ_BREA11 BZ_BREA46	BZ_BREA53 BZ_BREA63 BZ_BREA71 BZ_BREA81 BZ_BREA82 BZ_BREA86 BZ_BREA88 BZ_BREA96 BZ_BREA98 BZ_BREA9909
	do

	#print name of sample analysing
			echo now analysing $opt
	
	#make directory for out put files of gstacks
			mkdir /storage/home/users/elc6/stacksv2/gstacks/${opt}
	
	# convert from sam to bam
			$samtools
			$samtools view -h -S -b /storage/home/users/elc6/stacksv2/align/${opt}_aln.sam -o /storage/home/users/elc6/stacksv2/align/${opt}_aln.bam
	
	# sort reads using samtools
			$samtools	
			$samtools sort /storage/home/users/elc6/stacksv2/align/${opt}_aln.bam -o /storage/home/users/elc6/stacksv2/align/${opt}_alnS.bam

	## add read groups so that it can be analysed by gstacks withouth popmap
	java -jar /usr/local/Modules/modulefiles/tools/picard-tools/2.14.1/picard.jar AddOrReplaceReadGroups \
      I=/storage/home/users/elc6/stacksv2/align/${opt}_alnS.bam \
      O=/storage/home/users/elc6/stacksv2/align/${opt}_alnSRG.bam \
      RGID=HJF3HB \
      RGLB=${opt}lib1 \
      RGPU=HJF3HBBXX \
      RGPL=illumina \
      RGSM=${opt}
      
      ## analyse single file with gstacks to get alignment and loci statistics: not necessary but can be interesting
  #    gstacks --paired -B /storage/home/users/elc6/stacksv2/align/${opt}_alnSRG.bam \
  #    -O /storage/home/users/elc6/stacksv2/gstacks/${opt} \
   #   --max-clipped 0.1 --min-mapq 10
      
done

################################################################################################################################################

Now, all the samples are in the 'align' folder, along with 20 each from 4 other sampling locations and the replicate samples. The
population of origin is defined in what stacks calls the popmap file.

I am going to run gstacks and then populations separately.

gstacks code:
this varies several of the pipeline parameters to see how the loci calls co-vary with these
################################################################################################################################################
#!/bin/bash 
#$ -cwd 
#$ -j y
#$ -S /bin/bash 
#$ -V
#$ -N Trial_4Vars ## job name

#$ -q all.q

# load stacks
module load stacks/2.0Beta9

			## set --min-mapq variables to loop through: minimum mapping quality
			for var_m in 10 20  
				do
				
				## set --max-clipped variable to loop through: maximum soft clipping proportion
				for var_c in 0.1 0.2 
						do
				
					## set --var-alpha variable to loop through: alpha for calling SNPs
					for var_valpha in 0.05 0.01 
							do
						
						## set --gt-alpha variable to loop through: alpha for calling genotypes
						for var_gta in 0.05 0.01 
							do
							
							#print samples and parameter combo under analysis so its easy to see where you are at in the log file
							echo Trial Samples and Replicates mapq: $var_m max clipped: $var_c var alpha: $var_valpha gt alpha: $var_gta
				
							## make directory to hold output
							mkdir /storage/home/users/elc6/stacksv2/gstacks/Trial_mapq${var_m}_clip${var_c}_varalpha${var_valpha}_gtalpha${var_gta}
     			
     						## create loci catalogue using gstacks
      						gstacks -M ./stacksv2/popmap_refcat_Reps.txt \
      							-I ./stacksv2/align/ \
								-O ./stacksv2/gstacks/Trial_mapq${var_m}_clip${var_c}_varalpha${var_valpha}_gtalpha${var_gta}/ \
								--max-clipped ${var_c} --min-mapq ${var_m} --var-alpha ${var_valpha} --gt-alpha ${var_gta} -t 3
					done
				done
			done
		done
################################################################################################################################################

The next step is to create a dataset based on the 100 'reference' samples and then use this set of filtered loci to call the repeats
and estimate an error rate. This is done in bash script on the cluster, switching between R, vcftools and stacks. I am just working
this out and will loop it in the future, but this is the outline. Some of this code comes from Chris Hollenbeck https://github.com/chollenbeck

################################################################################################################################################

## load programs, and newer version of vcftools: an older one is loaded with dDocent
module load stacks/2.0Beta9	
module load vcflib/gitv0_e3ab177
module load dDocent/gitv0_7dea314
module unload vcftools/0.1.11 
module load vcftools/0.1.12a

## use populations to pull all loci called by gstacks that are variable; minor allele frequency of 0.01 means that at least one
## individual in 100 has a variable site. This is just the beginning, so it's just to get the variable loci.
## the popmap file only has the 100 reference samples in it

populations -P ./stacksv2/gstacks/Trial_mapq10_clip0.1_varalpha0.01_gtalpha0.01/ \
	-M ./stacksv2/gstacks/RefmapTrialVcf/popmap_refcat_RepsV3.txt \
	-O ./stacksv2/gstacks/RefmapTrialVcf --vcf -t 16 --min_maf 0.01

## switch to R and load programs needed
R

library(tidyverse)
library(stringr)

## Step 1: Initial filtering by quality and missing data
# Filter genotypes with genotype quality < 20
system("vcftools --vcf populations.snps.vcf --out out.1 --minGQ 20 --recode --recode-INFO-all")

# Filter out sites that were made monomorphic by the previous filter
system("vcftools --vcf out.1.recode.vcf --maf 0.001 --out out.2 --recode --recode-INFO-all")

# Remove sites with more than 50% missing data
system("vcftools --vcf out.2.recode.vcf --out out.3 --max-missing 0.5 --recode --recode-INFO-all")

# Cannot remove loci with extreme allele balance or filter by SNP quality as CH did
# but this is accounted for by the genotype calling algorithm used by Stacks

# Produce a file with missingness per individual
system("vcftools --vcf out.3.recode.vcf --out out.3 --missing-indv")

## Step 2: Filtering inidvidual samples by missing data
#Plot missingness
# Load the data for the missingness file and sort data by population
out_3_imiss <- read_tsv("out.3.imiss")
pop<-c(rep("NZ", 20), rep("AUS", 20), rep("BRZ", 20), rep ("ARG", 20), rep("SAF", 20))
missingness<-cbind(out_3_imiss, pop)
missplot<-melt(missingness[,c(1,5,6)])
Pops<-c("AUS", "NZ", "ARG", "BRZ", "SAF")
missplot<-within(missplot,pop<-factor(pop,levels=Pops))

# set colours for plotting
myPal4 <- c("green" ,"blue", "yellow", "pink", "orange")

mp<-ggplot(data=missplot, aes(x=INDV, y=value, fill=pop))
mp<-mp+geom_bar(stat = "identity")+scale_y_continuous(labels=percent_format())+scale_fill_manual(values = myPal4)
mp<-mp+theme(axis.text.x = element_text(size=6, angle = 90),
           axis.text.y = element_text(size=10),
           legend.text=element_text(size=10),
           legend.title=element_blank())

#check it plotted OK
mp

# save as pdf
pdf("missingness_by_pop.pdf")
mp
dev.off()

# Select individuals with more than 50% missing data
miss_50 <- filter(out_3_imiss, F_MISS > 0.5) %>%
            select(INDV)

# Write the individuals to remove to a file
write_delim(miss_50, "remove.3.inds", col_names = FALSE)

# Remove individuals with >50% missing data
system("vcftools --vcf out.3.recode.vcf --out out.4 --remove remove.3.inds --recode --recode-INFO-all")

## Step 3: Filtering loci with high read depth and missing data
# Calculate site depth
system("vcftools --vcf out.4.recode.vcf --site-depth --out out.5")

# Read in the site depth file and calculate mean depth divided by number of samples
site_depth_5 <- read_tsv("out.5.ldepth") %>%
                  mutate(MEAN_DEPTH = SUM_DEPTH / 80)

# Plot a histogram of the mean site depth per locus
qplot(site_depth_5$MEAN_DEPTH, binwidth = 10)

# Filter out loci with a mean site depth > 3x the overall mean
mean_site_depth_5 <- mean(site_depth_5$MEAN_DEPTH)
to_keep_5 <- filter(site_depth_5, MEAN_DEPTH < 3 * mean_site_depth_5)
mean_site_depth_5_filt <- mean(to_keep_5$MEAN_DEPTH)

# Plot the distribution again
qplot(to_keep_5$MEAN_DEPTH)

# Make a list of the sites to filter
to_filter_5 <- filter(site_depth_5, MEAN_DEPTH >= 3 * mean_site_depth_5) %>%
                  select(CHROM, POS)

# Write the sites to remove to a file
write_delim(to_filter_5, "remove.5.sites", col_names = FALSE)

# Remove the sites with VCFtools
system("vcftools --vcf out.4.recode.vcf --out out.5 --exclude-positions remove.5.sites --recode --recode-INFO-all")

# Calculate individual missingness
system("vcftools --vcf out.5.recode.vcf --out out.6 --missing-indv")
       
# Load the data for the out.6.recode.vcf file
out_6_imiss <- read_tsv("out.6.imiss")

# Plot a quick histogram of the data
qplot(out_6_imiss$F_MISS)

# Remove indels
system("vcftools --vcf out.5.recode.vcf --out out.6 --remove-indels --recode --recode-INFO-all")

## This gives us a 'whitelist' of loci to give back to stacks, and then filter using occurence, minor allele
## frequency and examine hwe, as well as estimating error rate

## the issue is that the stacks loci ID is not the same as variable site that we've been using
## it's the third column, so we cut it out, remove underscore and write the data to a tab delimited file
system("cut out.6.recode.vcf -f 3 > whitelist")
system("sed -e 's/_/\t/g' whitelist > whitelistv2")
system("sed '1,15d' whitelistv2 > whitelistv3")

# create folder for white list loci to go
mkdir WL

## use this in populations to take whitelist and ensure loci are found in 80% of samples from each of the 4 populations
## also apply minor allele frequency of 0.01 and record HWE information
## have to restart the session because of problems with loaded programs

## this could be done in a loop but as I was recording the results as I went I just did it by hand

##populations -P ./stacksv2/gstacksMay18/Trial_mapq20_clip0.1_varalpha0.05_gtalpha0.05/ \ 
##-M ./stacksv2/gstacksMay18/popmap_refcat_May80plusReps.txt \
##-O ./stacksv2/gstacksMay18/Trial_mapq20_clip0.1_varalpha0.05_gtalpha0.05/WL/ \
##-W ./stacksv2/gstacksMay18/Trial_mapq20_clip0.1_varalpha0.05_gtalpha0.05/whitelistv3 \
##-p 4 -r 0.8 --vcf --hwe --min_maf 0.01

## navigate to WL folder

# Produce a file with missingness per individual
system("vcftools --vcf populations.snps.vcf --out out.1 --missing-indv")

# Load the data for the missingness file
out_1_imiss <- read_tsv("out.1.imiss")
pop<-c(rep("NZ", 20), rep("AUS", 20), rep("BRZ", 20), rep ("ARG", 20), rep("Rep", 8))
missingness<-cbind(out_1_imiss, pop)
missplot<-melt(missingness[,c(1,5,6)])
Pops<-c("AUS", "NZ", "ARG", "BRZ", "Rep")
missplot<-within(missplot,pop<-factor(pop,levels=Pops))

myPal4 <- c("green" ,"blue", "yellow", "pink", "black")

q<-ggplot(data=missplot, aes(x=INDV, y=value, fill=pop))
q<-q+geom_bar(stat = "identity")+scale_y_continuous(labels=percent_format())+scale_fill_manual(values = myPal4)
q<-q+theme(axis.text.x = element_text(size=6, angle = 90),
           axis.text.y = element_text(size=10),
           legend.text=element_text(size=10),
           legend.title=element_blank())

pdf("final_missingness_per_sample.postQC.pdf")
q
dev.off()

# calculate mean amount of missing data per sample and its standard deviation
mean(missingness$F_MISS)
sd(missingness$F_MISS)

## Plot coverage per locus as well
# Calculate site depth
system("vcftools --vcf populations.snps.vcf --site-depth --out out.2")

# Read in the site depth file and calculate mean depth divided by number of samples (88 because of duplicates)
site_depth_2 <- read_tsv("out.2.ldepth") %>%
                  mutate(MEAN_DEPTH = SUM_DEPTH / 87)

# Plot a histogram of the mean site depth per locus
pdf("final_mean_depth_per_site.pdf")
qplot(site_depth_2$MEAN_DEPTH, binwidth = 10)
dev.off()

## calculate mean and standard deviation of mean site depth per locus per sample
mean(site_depth_2$MEAN_DEPTH)
sd(site_depth_2$MEAN_DEPTH)

## calculating error rates
dups_a<-c("NZ_Eau09NZ19_alnSRG", "NZ_Eau08AI165_alnSRG", "AUS_Eau95WA14_alnSRG",
			"AUS_Eau95WA17_alnSRG", "AUS_Eau14HOB09_alnSRG",
			"BZ_BREA85_alnSRG", "AUS_Eau12TAS05_alnSRG")
			
## and their duplicates
dups_b<-c("NZ_Eau09NZ19R_alnSRG", "NZ_Eau08AI165R_alnSRG", "AUS_Eau95WA14R_alnSRG",
			"AUS_Eau95WA17R1_alnSRG", "AUS_Eau14HOB09R_alnSRG", 
			"BZ_BREA85R_alnSRG", "AUS_Eau12TAS05R_alnSRG")

# write the names to file
write_delim(as.data.frame(dups_a), "dups_a.txt", col_names = FALSE)
write_delim(as.data.frame(dups_b), "dups_b.txt", col_names = FALSE)

			
#filter to create one vcf file of replicates and one of duplicates
system("vcftools --vcf populations.snps.vcf --out dups.a --keep dups_a.txt --recode --recode-INFO-all")
system("vcftools --vcf populations.snps.vcf --out dups.b --keep dups_b.txt --recode --recode-INFO-all")

# need to rename samples so that they are the same - did manually
system("nano dups.b.recode.vcf")

# then compare the two files to identify discordant loci
system("vcftools --vcf dups.a.recode.vcf --diff dups.b.recode.vcf --diff-site-discordance --out out.3.rep")

# this can be used to identify loci to exclude and estimate error rate during the 'real' run

discord <- read_tsv("out.3.rep.diff.sites")

## per allele error rate
sum(discord$N_DISCORD)
sum(discord$N_COMMON_CALLED)
sum(discord$N_DISCORD)/sum(discord$N_COMMON_CALLED)

## error free variable sites
table(discord$N_DISCORD)[1]		

## stacks does a HWE test in the populations commnad and we can use the output to look at how many sites and loci deviated from expectations
out.hwe<-read.table("populations.sumstats.tsv")
colnames(out.hwe)<-c("LocusID", "CHROM", "POS", "Col", "Pop", "PNuc", "QNuc", "N", "P", "ObsHet",
"ObsHom", "ExpHet", "ExpHom", "Pi", "SmPi", "SmPi_p", "SmFis", "SmFis_p", "HWE", "HWE_p", "private")

## pull out the HWE estimate if it < 0.01 and its stacks ID and variable site location
excl.hwe<-filter(out.hwe, HWE_p<0.01) %>%
	select(LocusID, CHROM, POS)
	
# number of variable sites out of hwe at p = 0.01
nrow(unique(excl.hwe))

# number of loci out of hwe at p = 0.01
length(unique(excl.hwe[,1]))
	
# not necessarily removing them, but good to have a record
write_delim(excl.hwe, "remove.hwe.sites", col_names = FALSE)

################################################################################################################################################

I then compared the output in terms of number of variable sites that made the final QC, proportion of missing data, depth and error rates
and decided that the best combination of parameters for the right whale dataset was: mapq = 10, clipq = 0.1, gt-alpha = 0.01 and SNP-alpha
=0.01. The proportion of error free variable sites was >98% and the error rate was 0.003 per allele 
(there were 847 discordant alleles in >320,000 called).





